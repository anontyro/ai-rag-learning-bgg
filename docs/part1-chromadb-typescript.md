# Part 1 — ChromaDB + TypeScript Learning Path (Boardgames RAG)

Audience: You have ChromaDB and Ollama running locally via Docker Compose and a small Express API (see `apps/api/index.ts`).
Outcome: Become productive with ChromaDB and retrieval in Node/TypeScript, using the boardgames dataset as the anchor. Prepare to bridge to local models with Ollama in Part 2.

How to use this doc:
- Each module has objectives, short readings, hands-on tasks, and a checkpoint.
- Keep your notes and decisions (e.g., chunk size, top-k) in a separate scratch file so you can iterate intentionally.

## Module 0 — Foundations

Objectives:
- Understand what vector databases do and where they fit in RAG.
- Grasp embeddings, distance metrics, and the retrieval step.

Readings:
- Concept: Vector databases, embeddings, similarity search (cosine/Euclidean/inner product).
- RAG overview: Retrieval feeding context into a generator.

Hands-on:
- Sketch the RAG pipeline you’re targeting: Ingest → (optional) Embed → Store in Chroma → Query → Rank/Filter → Feed to LLM (Part 2).

Checkpoint:
- You can explain why a vector DB is used instead of keyword search, and what a “top-k similarity search” returns.

## Dataset context — Boardgames (anchor for all modules)

We will use `data/boardgames_enriched.csv` generated by `apps/api/scripts/enrich-boardgames.ts`. Each row includes:
- `id`, `name`, `yearpublished`, global `rank` and per-genre ranks (e.g., `familygames_rank`, `strategygames_rank`, `wargames_rank`, etc.).
- Enriched text: `primaryName`, `description`.
- Attributes: `minplayers`, `maxplayers`, `playingtime`, `minage`, `is_expansion`.
- Tags: `categories`, `mechanics` (semicolon-separated strings).

Target collection name in Chroma: `boardgames`.

Example validation queries throughout this doc:
- “Top 10 family games” → sort by `familygames_rank` asc.
- “Sci‑fi wargames suggestions” → filter `categories contains 'Science Fiction'` and prefer items with `wargames_rank`.
- “Co‑op legacy games like Pandemic” → semantic match on description with cooperative/legacy hints.

## Module 1 — ChromaDB and Ollama in Docker (Local)

Objectives:
- Understand Chroma server vs. client, persistence, and your local stack.

Readings:
- Chroma server concepts: collections, documents, embeddings, metadata.
- Review your `docker-compose.yml` services:
  - `chromadb` on `http://localhost:8000` (REST)
  - `ollama` on `http://localhost:11434` (local models)
  - Persistent storage examples (e.g., SQLite under `getting-started/chroma.sqlite3`, Ollama volume for model cache)

Hands-on:
- Start services: `docker compose up -d chromadb ollama`.
- Verify Chroma: `curl http://localhost:8000/api/v1/collections`.
- Verify Ollama: `curl http://localhost:11434/api/tags`.
- Decide your URLs and document them in `.env` for the API (`CHROMA_URL`, `OLLAMA_URL`).

Checkpoint:
- From a Node REPL or script, you can `heartbeat` the Chroma server.

## Module 2 — TypeScript Client Basics

Objectives:
- Connect to Chroma from Node/TS and perform CRUD on collections.

Readings:
- Chroma TypeScript client usage (`chromadb` package) and basic methods: `ChromaClient`, `getOrCreateCollection`, `add`, `upsert`, `get`, `query`, `delete`.
- Collection options (e.g., specifying an embedding function or supplying vectors).

Hands-on:
- Create a `scripts/` directory and add simple TS scripts:
  - `scripts/create-collection.ts`: `getOrCreateCollection("docs")`.
  - `scripts/seed.ts`: add a few test documents with `ids`, `documents`, and simple `metadatas`.
  - `scripts/query.ts`: run a `query` with a small `n_results` (e.g., 3) and print ids, scores, and snippets.
- Configure the client with a base URL from env (e.g., `CHROMA_URL=http://localhost:8000`).

Checkpoint:
- You can create a collection, add items, and retrieve similar items with `query`.

## Module 3 — Embeddings in Practice

Objectives:
- Choose an embedding strategy and ensure consistent embeddings across ingest and query.

Readings:
- Embedding options: hosted APIs vs. local embeddings.
- Local-first preview (for Part 2): generating embeddings with local models and passing vectors to Chroma.

Hands-on (choose one for now; you can switch in Part 2):
- Option A (focus on Chroma first): Skip embeddings for now; rely on structured filters and later add vectors.
- Option B (local-first preview via Dockerized Ollama): Use Ollama `/api/embeddings` (e.g., `nomic-embed-text`) to generate vectors and pass them to Chroma. Track model name and dimensionality.

Key considerations:
- Chunking strategy (e.g., ~500–1,000 tokens with overlap) and stable model/version across ingest/query.
- Store embedding metadata (model name/version) at collection level or metadata for reproducibility.

Checkpoint:
- You can embed a small set of texts and store vectors in Chroma, then query with an embedded user query and see sensible results.

## Module 4 — Querying & Ranking

Objectives:
- Use similarity search effectively, interpret scores, and apply filters.

Readings:
- Distance metrics used by your embedding+Chroma setup.
- `where` and `whereDocument` filters to narrow results by metadata or content.

Hands-on:
- Structured (boardgames): implement queries for
  - Top 10 family games: sort by `familygames_rank` (ascending, non-zero/non-null first).
  - Filters: categories contains `"Science Fiction"`, mechanics contains `"Deck, Bag, and Pool Building"`, `minplayers <= 2 && maxplayers >= 2`, `playingtime <= 60`.
- If embeddings are enabled: compare pure semantic vs. hybrid (filter first → vector query) and tune `n_results`.
- Print score distributions to understand sensitivity to query phrasing.

Checkpoint:
- You can justify your default `top-k` and filtering choices for your dataset.

## Module 5 — Persistence, Scaling, and Hygiene

Objectives:
- Understand storage, backups, and collection design.

Readings:
- Persistence backends overview (e.g., SQLite for local dev), implications for backups and portability.
- Namespacing and multi-tenant patterns via collection names and metadata.

Hands-on:
- Decide a collection naming scheme (e.g., `docs:{env}`) and commit it to your README.
- Add scripts to export/import a collection (IDs + documents + metadata) for quick resets.

Checkpoint:
- You can wipe and rebuild a collection deterministically from your seed scripts.

## Module 6 — Wire Chroma into the Express API

Objectives:
- Expose minimal endpoints to ingest and search so other components (Part 2) can consume retrieval easily.
- Ground endpoints on the `boardgames` collection for practical tests.

Proposed endpoints in `apps/api/index.ts`:
- POST `/collections/:name/ingest`
  - Body: `{ documents: string[], metadatas?: object[], ids?: string[], embeddingModel?: string }`
  - Behavior: chunk (if requested), embed (A or B from Module 3), `upsert` into collection.
  - Returns: `{ upserted: number }`.
- GET `/collections/:name/search`
  - Query: `q` (string), `k` (number, default 5), `where` (JSON string), `whereDocument` (JSON string)
  - Behavior: embed the query (if vectors available), run `query`, return `{ ids, documents, metadatas, distances/scores }`.
- GET `/games/top`
  - Query: `categoryRankField` (e.g., `familygames_rank`), `limit` (default 10)
  - Behavior: fetch top N by rank from `boardgames` using metadata sort or precomputed list.

Hands-on:
- Add simple validation, error handling, and config for `CHROMA_URL`.
- Use your Module 3 embedding choice inside these endpoints.
- Test with curl or an HTTP client.

Checkpoint:
- You can ingest a few docs and retrieve relevant snippets over HTTP locally.

## Module 7 — Evaluate Retrieval Quality

Objectives:
- Build a lightweight feedback loop to improve retrieval before adding a generator.

Hands-on:
- Create a tiny golden set (queries → expected passages/ids).
- Measure hit rate@k; iterate on chunking, top-k, model choice.
- Log failures (queries with poor matches) for follow-up.

Checkpoint:
- You have a short list of decisions (chunk sizes, top-k, embedding model) with a rationale based on experiments.

---

## Deliverables by end of Part 1
- Dataset-driven artifacts:
  - `apps/api/scripts/enrich-boardgames.ts` (provided) to keep `data/boardgames_enriched.csv` fresh in batches.
  - `apps/api/scripts/ingest-boardgames.ts` to load `boardgames_enriched.csv` into Chroma collection `boardgames`.
- Scripts in `scripts/` for create/seed/query (for learning/testing outside the API if desired).
- Minimal Express endpoints: `/version`, `/db/heartbeat`, `/collections/:name/ingest`, `/collections/:name/search`, `/games/top`.
- A small golden set and notes on your retrieval configuration (top‑k, filters, model choice if using embeddings).

## Milestone 2 — Ollama + Embeddings + Hybrid Retrieval (Local, Boardgames)

Objectives:
- Run Ollama in Docker and select an embedding model suitable for recommendations.
- Embed `boardgames` documents and enable vector similarity in Chroma.
- Implement hybrid retrieval: combine metadata filters (players/time/categories) with vector search; tie‑break using ranks.
- Compare model and parameter choices (top‑k, filters, fusion) on a small golden set.

Readings:
- Ollama HTTP API (embeddings & generation): https://github.com/ollama/ollama/blob/main/docs/api.md
- Embedding models overview: `nomic-embed-text`, `mxbai-embed-large` (quality vs. speed tradeoffs).
- Chroma query parameters: `where`, `whereDocument`, vector similarity and distances.

Hands-on:
- Verify Dockerized Ollama is up and models are available:
  - `docker compose up -d ollama`
  - `docker exec -it ollama ollama pull nomic-embed-text`
  - Optional LLM for later: `docker exec -it ollama ollama pull llama3.1:8b`
- Embed and ingest:
  - Run: `pnpm ingest:games:small` to validate pipeline, then `pnpm ingest:games:embed` for ~1k rows.
  - Track embedding model name and dimensionality at collection level (notes/README).
- Add hybrid retrieval in `apps/api/index.ts`:
  - Implement a helper `hybridSearch()` that:
    - Embeds the user query via `OLLAMA_URL/api/embeddings`.
    - Calls `collection.query` with `where` filters from request (e.g., categories/mechanics, `minplayers<=x`, `maxplayers>=y`, `playingtime<=z`, `is_expansion=false`).
    - Uses `n_results` in the 8–20 range; return ids, scores, docs, metadatas.
    - Optionally tie‑break by rank field if provided (e.g., `familygames_rank`).
  - Expose GET `/games/search` with query params:
    - `q`, `k`, `categories`, `mechanics`, `players`, `maxTime`, `rankField`, `expansion` (default false).
- Compare retrieval modes on a few queries:
  - Pure semantic vs. metadata‑filtered vs. hybrid.
  - Measure top‑k hit rate on the golden set below.

Golden set (initial draft, extend as you go):
- Q1: "Top 10 family games" → Expect many with low `familygames_rank`.
- Q2: "I want sci‑fi wargames" → Filter category contains `Science Fiction` and prefer `wargames_rank`.
- Q3: "Best 2‑player under 45 minutes" → `minplayers<=2 && maxplayers>=2` and `playingtime<=45`.
- Q4: "Cooperative legacy games like Pandemic" → Semantic similarity on description with cooperative/legacy cues.

Deliverables:
- Embedding backfill complete for the `boardgames` collection (at least 1k rows).
- `GET /games/search` in `apps/api/index.ts` using hybrid retrieval and returning:
  - `{ ids, documents, metadatas, distances, usedFilters, model }`.
- Notes in `docs/` on chosen embedding model and parameter defaults (k, filters, fusion logic).

Acceptance criteria:
- Chroma returns meaningful semantic neighbors for dataset queries (manual spot‑check several results).
- Hybrid retrieval answers:
  - "Top 10 family games" with relevant titles and good rank ordering.
  - "Sci‑fi wargames suggestions" with sci‑fi category present and wargame relevance.
  - "Best 2‑player under 45 min" meeting numeric constraints.
- Latency acceptable on local machine (e.g., < 500ms for k=10 after warmup on ~1k docs; adjust to your hardware).

Tips:
- Normalize categories/mechanics (trim, consistent casing) during ingest to make filters reliable.
- Exclude expansions by default (`is_expansion=false`) unless explicitly requested.
- Keep a simple score fusion: sort by vector similarity; if `rankField` provided, stable sort ties by ascending rank.

## Milestone 3 — RAG Chat Endpoint (API bridge to Ollama)

Objectives:
- Build a `/chat` endpoint in `apps/api/index.ts` that performs RAG over the `boardgames` collection.
- Reuse hybrid retrieval from Milestone 2, assemble a concise prompt, and generate with Ollama LLM.
- Return grounded answers with citations (ids/names/ranks) and support streaming text responses.
- Provide a fast path for purely structured requests (e.g., "Top 10 family games") without invoking the LLM.

Readings:
- Ollama Generate API (streaming, parameters): https://github.com/ollama/ollama/blob/main/docs/api.md
- Prompting for recommendations and citations (short, factual, grounded outputs).
- Token budgeting basics (keep context small; use top‑K 6–10, short snippets).

Hands-on:
- Retrieval helper:
  - Extract `hybridSearch()` (from Milestone 2) into a module and import in the API.
  - Input: `{ q, k, filters, rankField }`; Output: `{ results, usedFilters }` with ids, documents, metadatas, distances.
- Prompt template:
  - Create `apps/api/prompts/recommendations.txt` with guidance:
    - Role: expert boardgame recommender.
    - Use only provided context; avoid hallucinations.
    - Format: short bullet points (title — why), then "Sources" list with ids/titles.
    - Respect constraints: players/time/categories/mechanics, exclude expansions by default.
- API endpoint:
  - POST `/chat`
    - Body: `{ q: string, k?: number, filters?: {...}, rankField?: string, stream?: boolean }`
    - Steps:
      1) If `rankField` is provided and `q` matches a known structured intent like "top" queries, short‑circuit: return top N from ranks with sources (no LLM).
      2) Else run `hybridSearch()` to get top‑K.
      3) Build a compact context payload for the prompt: for each result include `{id, name, key ranks, categories, mechanics (top 2), players, time}` and a 1–2 sentence snippet from `description`.
      4) Call Ollama generate (stream if requested) and return `{ answer, sources }`.
  - Streaming:
    - For Express, implement Server‑Sent Events (SSE) or chunked response. Start simple: non‑stream JSON first; add streaming as a follow‑up.
- Examples:
  - `curl -X POST http://localhost:3000/chat -H 'Content-Type: application/json' -d '{"q":"I want sci-fi wargames","filters":{"categories":["Science Fiction"]},"k":8}'`
  - `curl -X POST http://localhost:3000/chat -H 'Content-Type: application/json' -d '{"q":"Top 10 family games","rankField":"familygames_rank","k":10}'`

Deliverables:
- `POST /chat` in `apps/api/index.ts` that:
  - Accepts query + optional filters and rank preferences.
  - Uses hybrid retrieval (or structured fast path) and generates with Ollama.
  - Returns JSON: `{ answer: string, sources: Array<{ id: string, name: string, rank?: number }>, usedFilters, model }`.
- Prompt template file in `apps/api/prompts/` and a small test harness (curl examples/scripts).

Acceptance criteria:
- Answers include concise recommendations and a "Sources" section listing at least 3 items for k≥6.
- For "Top 10 family games" the endpoint can skip LLM and return the ranked list with short reasons (optional) and sources.
- For "Sci‑fi wargames" the response respects the sci‑fi filter and cites appropriate titles.
- For "Best 2‑player under 45 min" the response respects numeric constraints and cites appropriate titles.
- Reasonable latency locally (subject to model size); documentation includes guidance on selecting a smaller LLM if needed.

Tips:
- Truncate long descriptions in the prompt; prefer a short synthesized snippet per item.
- De‑duplicate near‑identical expansions unless explicitly requested.
- Include the embedding model and LLM names in the response for traceability.
- If retrieval returns empty, respond with a helpful clarification question rather than fabricating results.

## References & Resources
- ChromaDB Docs: https://docs.trychroma.com/
- chromadb NPM package: https://www.npmjs.com/package/chromadb
- Chroma GitHub: https://github.com/chroma-core/chroma
- Docker Docs (for local server & volumes): https://docs.docker.com/
- Transformers.js (local embeddings in Node): https://xenova.github.io/transformers.js/
- Sentence Transformers (concepts & models): https://www.sbert.net/
- Ollama (local models): https://ollama.com/

## Suggested Timeline
- Day 1–2: Modules 0–2
- Day 3: Modules 3–4
- Day 4: Modules 5–6
- Day 5: Module 7 + review and prepare for Part 2
